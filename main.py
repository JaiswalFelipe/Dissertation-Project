{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDSRdHhTksHIcibz63F57m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaiswalFelipe/Dissertation-Project/blob/main/main.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8Y2K7j07dl2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import datetime\n",
        "import pathlib\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import scipy.stats as stats\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, cohen_kappa_score, jaccard_score\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from config import *\n",
        "from utils import *\n",
        "\n",
        "from dataloader import DataLoader, NGDataset\n",
        "from networks.factory import model_factory\n",
        "\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "\n",
        "\n",
        "def test_full_map(test_loader, net, epoch, output_path):\n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    prob_im = np.zeros([test_loader.dataset.labels.shape[0],\n",
        "                        test_loader.dataset.labels.shape[1],\n",
        "                        test_loader.dataset.labels.shape[2], test_loader.dataset.num_classes], dtype=np.float32)\n",
        "    occur_im = np.zeros([test_loader.dataset.labels.shape[0],\n",
        "                         test_loader.dataset.labels.shape[1],\n",
        "                         test_loader.dataset.labels.shape[2], test_loader.dataset.num_classes], dtype=int)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterating over batches.\n",
        "        for i, data in enumerate(test_loader):\n",
        "            # Obtaining images, labels and paths for batch.\n",
        "            inps, labs, cur_maps, cur_xs, cur_ys = data\n",
        "\n",
        "            # Casting to cuda variables.\n",
        "            inps_c = Variable(inps).cuda()\n",
        "            # labs_c = Variable(labs).cuda()\n",
        "\n",
        "            # Forwarding.\n",
        "            outs = net(inps_c)\n",
        "            soft_outs = F.softmax(outs, dim=1)\n",
        "\n",
        "            for j in range(outs.shape[0]):\n",
        "                cur_map = cur_maps[j]\n",
        "                cur_x = cur_xs[j]\n",
        "                cur_y = cur_ys[j]\n",
        "\n",
        "                soft_outs_p = soft_outs.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
        "\n",
        "                prob_im[cur_map][cur_x:cur_x + test_loader.dataset.crop_size,\n",
        "                                 cur_y:cur_y + test_loader.dataset.crop_size, :] += soft_outs_p[j, :, :, :]\n",
        "                occur_im[cur_map][cur_x:cur_x + test_loader.dataset.crop_size,\n",
        "                                  cur_y:cur_y + test_loader.dataset.crop_size, :] += 1\n",
        "\n",
        "        # normalize to remove non-predicted pixels - if there is one\n",
        "        occur_im[np.where(occur_im == 0)] = 1\n",
        "\n",
        "        # calculate predictions\n",
        "        prob_im_argmax = np.argmax(prob_im / occur_im.astype(float), axis=-1)\n",
        "        # pixels with classes not used in the prediction are converted into 0\n",
        "        prob_im_argmax[np.where(test_loader.dataset.labels == 2)] = 0\n",
        "\n",
        "        for k, img_n in enumerate(test_loader.dataset.images):\n",
        "            # Saving predictions.\n",
        "            imageio.imwrite(os.path.join(output_path, img_n + '_pred_epoch_' + str(epoch) + '.png'),\n",
        "                            prob_im_argmax[k]*255)\n",
        "\n",
        "        lbl = test_loader.dataset.labels.flatten()\n",
        "        pred = prob_im_argmax.flatten()\n",
        "        print(lbl.shape, np.bincount(lbl.flatten()), pred.shape, np.bincount(pred.flatten()))\n",
        "\n",
        "        acc = accuracy_score(lbl, pred)\n",
        "        conf_m = confusion_matrix(lbl, pred)\n",
        "        f1_s_w = f1_score(lbl, pred, average='weighted')\n",
        "        f1_s_micro = f1_score(lbl, pred, average='micro')\n",
        "        f1_s_macro = f1_score(lbl, pred, average='macro')\n",
        "        kappa = cohen_kappa_score(lbl, pred)\n",
        "        jaccard = jaccard_score(lbl, pred)\n",
        "        tau, p = stats.kendalltau(lbl, pred)\n",
        "\n",
        "        _sum = 0.0\n",
        "        for k in range(len(conf_m)):\n",
        "            _sum += (conf_m[k][k] / float(np.sum(conf_m[k])) if np.sum(conf_m[k]) != 0 else 0)\n",
        "        nacc = _sum / float(test_loader.dataset.num_classes)\n",
        "\n",
        "        print(\"---- Validation/Test -- Epoch \" + str(epoch) +\n",
        "              \" -- Time \" + str(datetime.datetime.now().time()) +\n",
        "              \" Overall Accuracy= \" + \"{:.4f}\".format(acc) +\n",
        "              \" Normalized Accuracy= \" + \"{:.4f}\".format(nacc) +\n",
        "              \" F1 score weighted= \" + \"{:.4f}\".format(f1_s_w) +\n",
        "              \" F1 score micro= \" + \"{:.4f}\".format(f1_s_micro) +\n",
        "              \" F1 score macro= \" + \"{:.4f}\".format(f1_s_macro) +\n",
        "              \" Kappa= \" + \"{:.4f}\".format(kappa) +\n",
        "              \" Jaccard= \" + \"{:.4f}\".format(jaccard) +\n",
        "              \" Tau= \" + \"{:.4f}\".format(tau) +\n",
        "              \" Confusion Matrix= \" + np.array_str(conf_m).replace(\"\\n\", \"\")\n",
        "              )\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    return acc, nacc, f1_s_w, kappa, conf_m\n",
        "\n",
        "\n",
        "\n",
        "def validate(validation_loader, net, epoch, output_path):\n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    prob_im = np.zeros([validation_loader.dataset.labels.shape[0],\n",
        "                        validation_loader.dataset.labels.shape[1],\n",
        "                        validation_loader.dataset.labels.shape[2], validation_loader.dataset.num_classes], dtype=np.float32)\n",
        "    occur_im = np.zeros([validation_loader.dataset.labels.shape[0],\n",
        "                         validation_loader.dataset.labels.shape[1],\n",
        "                         validation_loader.dataset.labels.shape[2], validation_loader.dataset.num_classes], dtype=int)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterating over batches.\n",
        "        for i, data in enumerate(validation_loader):\n",
        "            # Obtaining images, labels and paths for batch.\n",
        "            inps, labs = data\n",
        "\n",
        "            # Casting to cuda variables.\n",
        "            inps_c = Variable(inps).cuda()\n",
        "            # labs_c = Variable(labs).cuda()\n",
        "\n",
        "            # Forwarding.\n",
        "            outs = net(inps_c)\n",
        "            soft_outs = F.softmax(outs, dim=1)\n",
        "\n",
        "            for j in range(outs.shape[0]):\n",
        "\n",
        "                soft_outs_p = soft_outs.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
        "                occur_vals = outs.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
        "\n",
        "                prob_im[:,:,:,:] += soft_outs_p[j, :, :, :]\n",
        "                occur_im[:,:,:,:] += occur_vals[j, :, :, :]\n",
        "\n",
        "        # normalize to remove non-predicted pixels - if there is one\n",
        "        occur_im[np.where(occur_im == 0)] = 1\n",
        "\n",
        "        # calculate predictions\n",
        "        prob_im_argmax = np.argmax(prob_im / occur_im.astype(float), axis=-1)\n",
        "        # pixels with classes not used in the prediction are converted into 0\n",
        "        prob_im_argmax[np.where(validation_loader.dataset.labels == 2)] = 0\n",
        "\n",
        "        for k, img_n in enumerate(validation_loader.dataset.images):\n",
        "            # Saving predictions.\n",
        "            imageio.imwrite(os.path.join(output_path, img_n + '_pred_epoch_' + str(epoch) + '.png'),\n",
        "                            prob_im_argmax[k]*255)\n",
        "\n",
        "        lbl = validation_loader.dataset.labels.flatten()\n",
        "        pred = prob_im_argmax.flatten()\n",
        "        print(lbl.shape, np.bincount(lbl.flatten()), pred.shape, np.bincount(pred.flatten()))\n",
        "\n",
        "        acc = accuracy_score(lbl, pred)\n",
        "        conf_m = confusion_matrix(lbl, pred)\n",
        "        f1_s_w = f1_score(lbl, pred, average='weighted')\n",
        "        f1_s_micro = f1_score(lbl, pred, average='micro')\n",
        "        f1_s_macro = f1_score(lbl, pred, average='macro')\n",
        "        kappa = cohen_kappa_score(lbl, pred)\n",
        "        jaccard = jaccard_score(lbl, pred)\n",
        "        tau, p = stats.kendalltau(lbl, pred)\n",
        "\n",
        "        _sum = 0.0\n",
        "        for k in range(len(conf_m)):\n",
        "            _sum += (conf_m[k][k] / float(np.sum(conf_m[k])) if np.sum(conf_m[k]) != 0 else 0)\n",
        "        nacc = _sum / float(validation_loader.dataset.num_classes)\n",
        "\n",
        "        print(\"---- Validation/Test -- Epoch \" + str(epoch) +\n",
        "              \" -- Time \" + str(datetime.datetime.now().time()) +\n",
        "              \" Overall Accuracy= \" + \"{:.4f}\".format(acc) +\n",
        "              \" Normalized Accuracy= \" + \"{:.4f}\".format(nacc) +\n",
        "              \" F1 score weighted= \" + \"{:.4f}\".format(f1_s_w) +\n",
        "              \" F1 score micro= \" + \"{:.4f}\".format(f1_s_micro) +\n",
        "              \" F1 score macro= \" + \"{:.4f}\".format(f1_s_macro) +\n",
        "              \" Kappa= \" + \"{:.4f}\".format(kappa) +\n",
        "              \" Jaccard= \" + \"{:.4f}\".format(jaccard) +\n",
        "              \" Tau= \" + \"{:.4f}\".format(tau) +\n",
        "              \" Confusion Matrix= \" + np.array_str(conf_m).replace(\"\\n\", \"\")\n",
        "              )\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    return acc, nacc, f1_s_w, kappa, conf_m\n",
        "\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    # Setting network for training mode.\n",
        "    model.train()\n",
        "\n",
        "    # Average Meter for batch loss.\n",
        "    train_loss = list()\n",
        "\n",
        "    # Iterating over batches.\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        inps, labels = data[0], data[1]\n",
        "\n",
        "        # if the current batch does not have samples from all classes\n",
        "        # print('out i', i, len(np.unique(labels.flatten())))\n",
        "        # if len(np.unique(labels.flatten())) < 10:\n",
        "        #     print('in i', i, len(np.unique(labels.flatten())))\n",
        "        #     continue\n",
        "\n",
        "        # Casting tensors to cuda.\n",
        "        inps = Variable(inps).cuda()\n",
        "        labs = Variable(labels).cuda()\n",
        "\n",
        "        # Clears the gradients of optimizer.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forwarding.\n",
        "        outs = model(inps)\n",
        "\n",
        "        # Computing loss.\n",
        "        loss = criterion(outs, labs)\n",
        "\n",
        "        if math.isnan(loss):\n",
        "            print('-------------------------NaN-----------------------------------------------')\n",
        "            print(inps.shape, labels.shape, outs.shape, np.bincount(labels.flatten()))\n",
        "            print(np.min(inps.cpu().data.numpy()), np.max(inps.cpu().data.numpy()),\n",
        "                  np.isnan(inps.cpu().data.numpy()).any())\n",
        "            print(np.min(labels.cpu().data.numpy()), np.max(labels.cpu().data.numpy()),\n",
        "                  np.isnan(labels.cpu().data.numpy()).any())\n",
        "            print(np.min(outs.cpu().data.numpy()), np.max(outs.cpu().data.numpy()),\n",
        "                  np.isnan(outs.cpu().data.numpy()).any())\n",
        "            print('-------------------------NaN-----------------------------------------------')\n",
        "            raise AssertionError\n",
        "\n",
        "        # Computing backpropagation.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Updating loss meter.\n",
        "        train_loss.append(loss.data.item())\n",
        "\n",
        "        # Printing.\n",
        "        if (i + 1) % DISPLAY_STEP == 0:\n",
        "            soft_outs = F.softmax(outs, dim=1)\n",
        "            # Obtaining predictions.\n",
        "            prds = soft_outs.cpu().data.numpy().argmax(axis=1).flatten()\n",
        "\n",
        "            labels = labels.cpu().data.numpy().flatten()\n",
        "\n",
        "            # filtering out pixels\n",
        "            coord = np.where(labels != train_loader.dataset.num_classes)\n",
        "            labels = labels[coord]\n",
        "            prds = prds[coord]\n",
        "\n",
        "            acc = accuracy_score(labels, prds)\n",
        "            conf_m = confusion_matrix(labels, prds, labels=[0, 1])\n",
        "            f1_s = f1_score(labels, prds, average='weighted')\n",
        "\n",
        "            _sum = 0.0\n",
        "            for k in range(len(conf_m)):\n",
        "                _sum += (conf_m[k][k] / float(np.sum(conf_m[k])) if np.sum(conf_m[k]) != 0 else 0)\n",
        "\n",
        "            print(\"Training -- Epoch \" + str(epoch) + \" -- Iter \" + str(i + 1) + \"/\" + str(len(train_loader)) +\n",
        "                  \" -- Time \" + str(datetime.datetime.now().time()) +\n",
        "                  \" -- Training Minibatch: Loss= \" + \"{:.6f}\".format(train_loss[-1]) +\n",
        "                  \" Overall Accuracy= \" + \"{:.4f}\".format(acc) +\n",
        "                  \" Normalized Accuracy= \" + \"{:.4f}\".format(_sum / float(train_loader.dataset.num_classes)) +\n",
        "                  \" F1 Score= \" + \"{:.4f}\".format(f1_s) +\n",
        "                  \" Confusion Matrix= \" + np.array_str(conf_m).replace(\"\\n\", \"\")\n",
        "                  )\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return sum(train_loss) / len(train_loss), _sum / float(train_loader.dataset.num_classes)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='main')\n",
        "    # general options\n",
        "    parser.add_argument('--operation', type=str, required=True, help='Operation [Options: Train | Test]')\n",
        "    parser.add_argument('--output_path', type=str, required=True,\n",
        "                        help='Path to to save outcomes (such as images and trained models) of the algorithm.')\n",
        "\n",
        "    # dataset options\n",
        "    parser.add_argument('--training_images_path', type=str, required=True, help='Dataset path.')\n",
        "    parser.add_argument('--training_masks_path', type=str, required=True, help='Dataset path.')\n",
        "    \n",
        "    parser.add_argument('--validation_images_path', type=str, required=True, help='Dataset path.')\n",
        "    parser.add_argument('--validation_masks_path', type=str, required=True, help='Dataset path.')\n",
        "\n",
        "    parser.add_argument('--testing_images_path', type=str, required=True, help='Dataset path.')\n",
        "    #parser.add_argument('--testing_masks_path', type=str, required=True, help='Dataset path.')\n",
        "    parser.add_argument('--testing_images', type=str, nargs=\"+\", required=True, help='Testing image names.')\n",
        "    \n",
        "    parser.add_argument('--crop_size', type=int, required=False, help='Crop size.')\n",
        "    parser.add_argument('--stride_crop', type=int, required=False, help='Stride size')\n",
        "\n",
        "    # model options\n",
        "    parser.add_argument('--model_name', type=str, required=True,\n",
        "                        choices=['deeplab', 'fcnwideresnet'], help='Model to evaluate')\n",
        "    parser.add_argument('--model_path', type=str, default=None, help='Model path.')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.01, help='Learning rate')\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.005, help='Weight decay')\n",
        "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
        "    parser.add_argument('--epoch_num', type=int, default=500, help='Number of epochs')\n",
        "\n",
        "    # handling imbalanced data\n",
        "    parser.add_argument('--loss_weight', type=float, nargs='+', default=[1.0, 1.0], help='Weight Loss.')\n",
        "    parser.add_argument('--weight_sampler', type=str2bool, default=False, help='Use weight sampler for loader?')\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "\n",
        "    # Making sure output directory is created.\n",
        "    pathlib.Path(args.output_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # writer for the tensorboard\n",
        "    writer = SummaryWriter(os.path.join(args.output_path, 'logs'))\n",
        "\n",
        "    if args.operation == 'Train':\n",
        "        print('---- training data ----')\n",
        "        train_set = NGDataset(args.img_path, args.mask_path, args.output_path)\n",
        "\n",
        "        #train_set = DataLoader('Train', args.dataset_path, args.training_images, args.crop_size, args.stride_crop,\n",
        "        #                       args.output_path)\n",
        "        \n",
        "        print('---- validation data ----')\n",
        "        validation_set = NGDataset(args.vali_img_path, args.vali_mask_path, args.output_path)\n",
        "\n",
        "        if args.weight_sampler is False:\n",
        "            train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size,\n",
        "                                                       shuffle=True, num_workers=NUM_WORKERS, drop_last=False)\n",
        "        else:\n",
        "            class_loader_weights = 1. / np.bincount(train_set.gen_classes)\n",
        "            samples_weights = class_loader_weights[train_set.gen_classes]\n",
        "            sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weights, len(samples_weights),\n",
        "                                                                     replacement=True)\n",
        "            train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size,\n",
        "                                                       num_workers=NUM_WORKERS, drop_last=False, sampler=sampler)\n",
        "\n",
        "        validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=args.batch_size,\n",
        "                                                  shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
        "\n",
        "        # Setting network architecture.\n",
        "        model = model_factory(args.model_name, train_set.num_channels, train_set.num_classes).cuda()\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(args.loss_weight),\n",
        "                                        ignore_index=train_set.num_classes).cuda()\n",
        "\n",
        "        # Setting optimizer.\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay,\n",
        "                               betas=(0.9, 0.99))\n",
        "        # optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, momentum=0.9)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
        "\n",
        "        curr_epoch = 1\n",
        "        best_records = []\n",
        "        if args.model_path is not None:\n",
        "            print('Loading model ' + args.model_path)\n",
        "            best_records = np.load(os.path.join(args.output_path, 'best_records.npy'), allow_pickle=True)\n",
        "            model.load_state_dict(torch.load(args.model_path))\n",
        "            # optimizer.load_state_dict(torch.load(args.model_path.replace(\"model\", \"opt\")))\n",
        "            curr_epoch += int(os.path.basename(args.model_path)[:-4].split('_')[-1])\n",
        "            for i in range(curr_epoch):\n",
        "                scheduler.step()\n",
        "        model.cuda()\n",
        "\n",
        "        # Iterating over epochs.\n",
        "        print('---- training ----')\n",
        "        for epoch in range(curr_epoch, args.epoch_num + 1):\n",
        "            # Training function.\n",
        "            t_loss, t_nacc = train(train_loader, model, criterion, optimizer, epoch)\n",
        "            writer.add_scalar('Train/loss', t_loss, epoch)\n",
        "            writer.add_scalar('Train/acc', t_nacc, epoch)\n",
        "            if epoch % VAL_INTERVAL == 0:\n",
        "                # Computing test.\n",
        "                acc, nacc, f1_s, kappa, track_cm = validate(validation_loader, model, epoch, args.output_path)\n",
        "                writer.add_scalar('Test/acc', nacc, epoch)\n",
        "                save_best_models(model, args.output_path, best_records, epoch, kappa)\n",
        "                # patch_acc_loss=None, patch_occur=None, patch_chosen_values=None\n",
        "            scheduler.step()\n",
        "    elif args.operation == 'Test':\n",
        "        print('---- testing data ----')\n",
        "        test_set = DataLoader('Test', args.dataset_path, args.training_images, args.crop_size, args.stride_crop,\n",
        "                              args.output_path)\n",
        "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size,\n",
        "                                                  shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
        "\n",
        "        # Setting network architecture.\n",
        "        model = model_factory(args.model_name, test_set.num_channels, test_set.num_classes).cuda()\n",
        "\n",
        "        best_records = np.load(os.path.join(args.output_path, 'best_records.npy'), allow_pickle=True)\n",
        "        index = 0\n",
        "        for i in range(len(best_records)):\n",
        "            if best_records[index]['kappa'] < best_records[i]['kappa']:\n",
        "                index = i\n",
        "        epoch = int(best_records[index]['epoch'])\n",
        "        print(\"loading model_\" + str(epoch) + '.pth')\n",
        "        model.load_state_dict(torch.load(os.path.join(args.output_path, 'model_' + str(epoch) + '.pth')))\n",
        "        model.cuda()\n",
        "\n",
        "        test_full_map(test_loader, model, epoch, args.output_path)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Process \" + args.operation + \"not found!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}